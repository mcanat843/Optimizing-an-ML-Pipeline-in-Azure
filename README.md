# Optimizing an ML Pipeline in Azure

## Overview
This project is part of the Udacity Azure ML Nanodegree.
In this project, we build and optimize an Azure ML pipeline using the Python SDK and a provided Scikit-learn model.
This model is then compared to an Azure AutoML run.

## Summary
**In 1-2 sentences, explain the problem statement: e.g "This dataset contains data about... we seek to predict..."**
-This dataset contains data about marketing campaigns on both current and retired employees of a marketing department of a bank. The created process seeks to predict if the employee will 
subscribe to the term deposit which is in the Y column of the Data Set.

**In 1-2 sentences, explain the solution: e.g. "The best performing model was a ..."**
- The accuracy of the hyperdrive run is approximately 91.0980 percent while the AutoML run has an approximately 91.6198 percent. Having these results, the best performing model was the run
using autoML with the VotingEnsemble algorithm.

## Scikit-learn Pipelines 
**Explain the pipeline architecture, including data, hyperparameter tuning, and classification algorithm.**
-The pipeline architechture has a series of steps and functions that again, seeks to predict if an employee will subscribe or not. The first step which is in the train.py is to create a 
TabularDataset from a given link. After creating the dataset, the data is hot encoded and cleaned using a defined function clean_data. After clean_data function, the data is split to train
and test sets in which I set the train size to 33 percent and random state to 7. The classification algorithm used was Logistic Regression in which predicts the probability of occurence of 
an event by fitting data into the function. In the hyper parameter tuning(ps), I used the RandomParameterSampling and set the ranges for the '--C' to .9 and 1.1 since it has a float value 
and the choices for the '--max_iter' to (80, 90, 100, 110, 120) since it has an int value. Lastly, the hyperdrive run was submitted with the RunDetails showed and the best run was saved. 

**What are the benefits of the parameter sampler you chose?**
-The main benefit of the random parameter sampling is that it will try different combination of values from the range that I have set for both the '--C' and the '--max_iter' parameters
to find the best combination that will maximize the primary metric.

**What are the benefits of the early stopping policy you chose?**
-The early termination policy will terminate the job if the primary metric falls outside of the top 10 percent in every 2 iterations to make sure that the process won't take too long 
and use up a lot of resources in trying to find out what the optimal parameter is.

## AutoML
**In 1-2 sentences, describe the model and hyperparameters generated by AutoML.**
-The AutoML experiment has a 30 minutes timeout and has a task named classification. The primary metric used is accuracy and the training data is the from the variable y_train which is from
the column y of the dataset. The AutoML run has 5 n cross validations. There are a lot of algorithms that has been tried and used by the automl run but the algorithms which got the highest
accuracy was the VotingEnsemble, which is equal to approx. 91.62 percent. Also, metrics are ranging from 72 percent to 91 percent in which the lowest accuracy was from the algorithm called
SparseNormalizer ExtremeRandomTrees.            

## Pipeline comparison
**Compare the two models and their performance. What are the differences in accuracy? In architecture? If there was a difference, why do you think there was one?**
-The difference in the two models is that the hyperdrive run that uses scikit learn needs and calls for another script, in this case the train.py to get the data and then set some 
parameters such as policy and estimator while the automl was fully set up on the jupyter notebook also with some parameters being set up such as primary_metric and experiment_timeout_minute.
The difference in the accuracy of the best runs of both the automl run and the hyperdrive run was very small. The best run in hyperdrive has a 91.0980 accuracy and the best run in
 automl was the VotingEnsemble algorithm, which has a 91.6198 accuracy, in which when calculated results only in a 0.5218 accuracy difference which I think is not significant.

## Future work
**What are some areas of improvement for future experiments? Why might these improvements help the model?**
-Build and sort the dataset according to criteria so that one can easily identify how many are the possible element are there in each criteria. In the project, the experiment_timeout_minutes
has been capped to only 30 minutes so there could only be a limited number of models that can be runned on that time period, so a longer timeout could have greater number of models to run
and thus improving the performance. Accuracy is the primary_metric used on the automl run in this project,but there are other primary metric that can be used on the automl configuration 
which is good depending on your own work cases, for example, balanced_accuracy which is a primary metric that calculates for the arithmetic mean of recall for each class. Another example
is AUC_weighted, which gets the arithmetic mean of the score for each class , weighted by the true number of true instances in each class.

## Proof of cluster clean up
**If you did not delete your compute cluster in the code, please complete this section. Otherwise, delete this section.**
**Image of cluster marked for deletion**
- The screenshot was also uploaded on the repository.
